{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習內容』\n",
    "#### 運用這幾天所學觀念搭建一個CNN分類器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習目的』\n",
    "  #### 熟悉CNN分類器搭建步驟與原理\n",
    "  #### 學員們可以嘗試不同搭法，如使用不同的Maxpooling層，用GlobalAveragePooling取代Flatten等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:30:55.017046Z",
     "start_time": "2020-03-28T07:30:47.945850Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:31:00.733889Z",
     "start_time": "2020-03-28T07:30:55.409821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape) #(50000, 32, 32, 3)\n",
    "\n",
    "## Normalize Data\n",
    "def normalize(X_train,X_test):\n",
    "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7) \n",
    "        return X_train, X_test,mean,std\n",
    "    \n",
    "    \n",
    "## Normalize Training and Testset    \n",
    "x_train, x_test,mean_train,std_train = normalize(x_train, x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:31:03.620794Z",
     "start_time": "2020-03-28T07:31:03.600806Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## OneHot Label 由(None, 1)-(None, 10)\n",
    "## ex. label=2,變成[0,0,1,0,0,0,0,0,0,0]\n",
    "one_hot=OneHotEncoder()\n",
    "y_train=one_hot.fit_transform(y_train).toarray()\n",
    "y_test=one_hot.transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T08:09:25.808609Z",
     "start_time": "2020-03-28T07:31:12.132545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=100)`\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 25s 502us/step - loss: 1.3869 - acc: 0.5289\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 22s 448us/step - loss: 0.8901 - acc: 0.6865\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 22s 447us/step - loss: 0.6563 - acc: 0.76838s - loss: 0.642\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 22s 448us/step - loss: 0.4625 - acc: 0.8375- ETA: 1s - loss: 0.4597 -\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 22s 443us/step - loss: 0.3145 - acc: 0.89190s - loss: 0.3121 - ac\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 22s 448us/step - loss: 0.2142 - acc: 0.9262\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 22s 448us/step - loss: 0.1612 - acc: 0.9455\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 22s 449us/step - loss: 0.1302 - acc: 0.9548\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.1063 - acc: 0.9641\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.1056 - acc: 0.9635\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 23s 458us/step - loss: 0.0835 - acc: 0.9714\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.0892 - acc: 0.9701\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.0756 - acc: 0.9745\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.0631 - acc: 0.9791\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.0582 - acc: 0.9807\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 24s 470us/step - loss: 0.0684 - acc: 0.97850s - loss: 0.0681 - acc: 0.9\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.0511 - acc: 0.9828\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.0508 - acc: 0.9836\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.0417 - acc: 0.9860\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.0471 - acc: 0.9849\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.0609 - acc: 0.9812\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.0502 - acc: 0.9841\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.0355 - acc: 0.9882\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.0406 - acc: 0.98640s - loss: 0.0404 - acc: 0.98\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.0487 - acc: 0.98475s - lo\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.0367 - acc: 0.9876\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.0412 - acc: 0.9869\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.0315 - acc: 0.9894\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.0241 - acc: 0.99203 - ETA: 1s - loss:\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.0331 - acc: 0.9891\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.0339 - acc: 0.9892\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.0367 - acc: 0.9886\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.0287 - acc: 0.99090s - loss: 0.0283 - acc: \n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.0285 - acc: 0.9909\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.0292 - acc: 0.9907\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.0304 - acc: 0.9910\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 0.0312 - acc: 0.9903\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.0228 - acc: 0.9928\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.0286 - acc: 0.9911\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.0216 - acc: 0.9933\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.0312 - acc: 0.9905\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.0243 - acc: 0.9919\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.0200 - acc: 0.9936\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.0200 - acc: 0.9935\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.0276 - acc: 0.9919\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.0226 - acc: 0.9926\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.0160 - acc: 0.99501s - lo\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 24s 486us/step - loss: 0.0283 - acc: 0.9912\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.0274 - acc: 0.9916\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.0187 - acc: 0.9945\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.0236 - acc: 0.9930\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.0186 - acc: 0.9941\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.0199 - acc: 0.9939\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 25s 491us/step - loss: 0.0183 - acc: 0.9944\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.0185 - acc: 0.9937\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 25s 494us/step - loss: 0.0199 - acc: 0.9939\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 25s 493us/step - loss: 0.0209 - acc: 0.9937\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.0131 - acc: 0.9962\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 25s 491us/step - loss: 0.0190 - acc: 0.9947\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.0187 - acc: 0.9944\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.0216 - acc: 0.9939\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 25s 497us/step - loss: 0.0206 - acc: 0.9940\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 24s 485us/step - loss: 0.0145 - acc: 0.9956\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 25s 496us/step - loss: 0.0191 - acc: 0.9941\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.0146 - acc: 0.9957\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.0128 - acc: 0.9961\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 24s 486us/step - loss: 0.0152 - acc: 0.9957\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 25s 491us/step - loss: 0.0189 - acc: 0.9943\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 25s 495us/step - loss: 0.0205 - acc: 0.9937\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.0140 - acc: 0.9956\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0161 - acc: 0.9953\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0179 - acc: 0.9949\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 21s 418us/step - loss: 0.0105 - acc: 0.9968\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 22s 433us/step - loss: 0.0098 - acc: 0.9974\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 21s 427us/step - loss: 0.0138 - acc: 0.99580s - loss: 0.0134 \n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 21s 421us/step - loss: 0.0234 - acc: 0.9936\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 22s 434us/step - loss: 0.0143 - acc: 0.9958\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 21s 418us/step - loss: 0.0191 - acc: 0.9945\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 22s 436us/step - loss: 0.0170 - acc: 0.9955\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 21s 418us/step - loss: 0.0106 - acc: 0.9965\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 21s 424us/step - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 21s 424us/step - loss: 0.0109 - acc: 0.9968\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 21s 421us/step - loss: 0.0091 - acc: 0.9971\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 21s 422us/step - loss: 0.0229 - acc: 0.9936\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 21s 420us/step - loss: 0.0141 - acc: 0.9955\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 21s 421us/step - loss: 0.0100 - acc: 0.9973\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 21s 421us/step - loss: 0.0087 - acc: 0.9975\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 21s 422us/step - loss: 0.0129 - acc: 0.99645s - los\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 21s 421us/step - loss: 0.0195 - acc: 0.9944\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 21s 421us/step - loss: 0.0120 - acc: 0.9964\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 21s 426us/step - loss: 0.0119 - acc: 0.9964\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 21s 423us/step - loss: 0.0108 - acc: 0.9965\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 21s 421us/step - loss: 0.0124 - acc: 0.9966\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 21s 421us/step - loss: 0.0099 - acc: 0.9972\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 21s 422us/step - loss: 0.0109 - acc: 0.9966\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 21s 423us/step - loss: 0.0132 - acc: 0.9960\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 21s 426us/step - loss: 0.0136 - acc: 0.99581s - loss\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 21s 426us/step - loss: 0.0121 - acc: 0.9965\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 21s 424us/step - loss: 0.0129 - acc: 0.9965\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 21s 423us/step - loss: 0.0117 - acc: 0.9966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2528996ef60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classifier=Sequential()\n",
    "\n",
    "#卷積組合\n",
    "classifier.add(Convolution2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (32, 32, 3)))#32,3,3,input_shape=(32,32,3),activation='relu''\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "\n",
    "#卷積組合\n",
    "classifier.add(Convolution2D(64, (3, 3), padding = 'same', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "#flatten\n",
    "classifier.add(Flatten())\n",
    "\n",
    "#FC\n",
    "classifier.add(Dense(output_dim = 100, activation = 'relu')) #output_dim=100,activation=relu\n",
    "\n",
    "#輸出\n",
    "classifier.add(Dense(output_dim=10,activation='softmax'))\n",
    "\n",
    "#超過兩個就要選categorical_crossentrophy\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(x_train,y_train,batch_size=100,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測新圖片，輸入影像前處理要與訓練時相同\n",
    "#### ((X-mean)/(std+1e-7) ):這裡的mean跟std是訓練集的\n",
    "## 維度如下方示範"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T08:13:43.439815Z",
     "start_time": "2020-03-28T08:13:42.877044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1214985e-06, 8.8122970e-06, 3.0843496e-06, 9.9505323e-01,\n",
       "        4.9327831e-03, 5.0912754e-09, 8.1658389e-13, 6.1654312e-08,\n",
       "        1.2705424e-09, 3.4285086e-13]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_example=(np.zeros(shape=(1,32,32,3))-mean_train)/(std_train+1e-7) \n",
    "classifier.predict(input_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
